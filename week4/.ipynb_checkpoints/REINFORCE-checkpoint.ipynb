{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "367d5855",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.nn import Module\n",
    "from torch.nn import Linear\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical\n",
    "from torch.optim import Adam\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0677592",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1e4dfd6dc30>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RANDOM_SEED = 99\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "97394b72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[99]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "env.seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "be0df45a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Свойста агента\n",
    "STATES_SIZE =env.observation_space.shape[0]\n",
    "ACTION_SIZE =env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "21def7b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNet(Module):\n",
    "    \n",
    "    def __init__(self,input_size,output_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.fc1=Linear(input_size,32)\n",
    "        self.fc2=Linear(32,output_size)\n",
    "        \n",
    "        self.train()\n",
    "        self.onpolicy_reset()\n",
    "    \n",
    "    def onpolicy_reset(self):\n",
    "        self.rewards=[]\n",
    "        self.log_probs=[]\n",
    "    \n",
    "    def forward(self,state):\n",
    "        \n",
    "        x=self.fc1(state)\n",
    "        x=F.relu(x)\n",
    "        x=self.fc2(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    # Функция политики, возвращает действие на састояние и \n",
    "    # логорифм действия\n",
    "    def act(self,state):\n",
    "        x=torch.from_numpy(state.astype(np.float32))\n",
    "        \n",
    "        props=self.forward(x) \n",
    "        prop_cat=Categorical(logits=props)\n",
    "        # Получаем действие\n",
    "        action=prop_cat.sample()\n",
    "        self.log_probs.append(prop_cat.log_prob(action))\n",
    "        return action.item()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7890571e",
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma=0.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c09b79f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, optimizer):\n",
    "    T=len(net.rewards)\n",
    "    rets = np.empty(T,dtype=np.float32)\n",
    "    futere_ret=0.0\n",
    "    \n",
    "    for t in reversed(range(T)):\n",
    "        #print(\"t:{}\".format(t))\n",
    "        # Берем награду и добавляем \n",
    "        futere_ret =net.rewards[t]+gamma*futere_ret\n",
    "        #print(\"futere_ret:{}\".format(futere_ret))\n",
    "        rets[t]=futere_ret\n",
    "        #print(\"rets:{}\".format(rets[t]))\n",
    "        #print(\"----------end-----------\")\n",
    "        \n",
    "    rets=torch.tensor(rets)\n",
    "    log_probs=torch.stack(net.log_probs)\n",
    "    loss= - log_probs*rets\n",
    "    loss=torch.sum(loss)\n",
    "    \n",
    "    optimizator.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizator.step()\n",
    "    \n",
    "    return loss\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c55fb437",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = PolicyNet(STATES_SIZE,ACTION_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "caf914cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizator = Adam(policy.parameters(),lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c8ac7ca8",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[1;32mIn [11]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m done:\n\u001b[0;32m     12\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m loss \u001b[38;5;241m=\u001b[39m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpolicy\u001b[49m\u001b[43m,\u001b[49m\u001b[43moptimizator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m total_reward\u001b[38;5;241m=\u001b[39m\u001b[38;5;28msum\u001b[39m(policy\u001b[38;5;241m.\u001b[39mrewards)\n\u001b[0;32m     16\u001b[0m solved \u001b[38;5;241m=\u001b[39m total_reward\u001b[38;5;241m>\u001b[39m\u001b[38;5;241m195.0\u001b[39m\n",
      "Input \u001b[1;32mIn [7]\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(net, optimizer)\u001b[0m\n\u001b[0;32m     18\u001b[0m loss\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39msum(loss)\n\u001b[0;32m     20\u001b[0m optimizator\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 21\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m optimizator\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\rl\\lib\\site-packages\\torch\\_tensor.py:363\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    354\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    355\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    356\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    357\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    361\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39mcreate_graph,\n\u001b[0;32m    362\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs)\n\u001b[1;32m--> 363\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\rl\\lib\\site-packages\\torch\\autograd\\__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    168\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    170\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    171\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    172\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 173\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward."
     ]
    }
   ],
   "source": [
    "for epoch in range(300):\n",
    "    state = env.reset()\n",
    "    \n",
    "    for t in range(200):\n",
    "        action = policy.act(state)\n",
    "        state, reward, done, _ = env.step(action)\n",
    "        policy.rewards.append(reward)\n",
    "        \n",
    "        env.render()\n",
    "        # Если игра завершинна\n",
    "        if done:\n",
    "            break\n",
    "    \n",
    "    loss =train(policy,optimizator)\n",
    "    total_reward=sum(policy.rewards)\n",
    "    solved = total_reward>195.0\n",
    "    policy.onpolicy_reset()\n",
    "    \n",
    "    print(\"Epoch:{} loss:{} total_reward:{} solved:{}\".format(epoch,loss,total_reward,solved))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f9d391",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
